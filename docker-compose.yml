
    version: '3.8'

    services:
      # 1. Servicio Backend (API FastAPI + Ollama)
      backend:
        build:
          context: ./product-query-bot-backend
          dockerfile: Dockerfile.backend
        ports:
          - "8000:8000"
        environment:
          OPENAI_API_KEY: your_openai_api_key_here
          TOP_K: 3
          VECTOR_STORE_PATH: ./faiss_index
          LOCAL_LLM_URL: http://ollama:11434
          LOCAL_LLM_MODEL: llama3
        depends_on:
          - ollama
        networks:
          - product_bot_network

      # 2. Servicio Frontend (Aplicación React)
      frontend:
        build:
          context: ./product-query-bot-frontend
          dockerfile: Dockerfile
        ports:
          - "5173:80"
        depends_on:
          - backend
        networks:
          - product_bot_network

      # 3. Servicio Ollama (Servidor LLM Local)
      ollama:
        image: ollama/ollama:latest
        ports:
          - "11434:11434"
        volumes:
          - ollama_data:/root/.ollama
        networks:
          - product_bot_network

    # Definición de la red y los volúmenes
    networks:
      product_bot_network:

    volumes:
      ollama_data:

    